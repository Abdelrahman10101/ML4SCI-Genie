1.# Custom Weighted MSE Loss  
   - Implements `WeightedMSE(weight_nonzero=1000.0)` to prioritize non-zero pixel reconstruction (critical for sparse jet images).  
   - Balances focus between sparse/dense regions by weighting non-zero pixels 1000x more than zeros.

2.# Efficient Data Handling 
   - Processes data in parallel using `ThreadPoolExecutor` with all available CPU workers (`num_workers=min(8, os.cpu_count())`).  
   - Uses only 30K samples (subset of full dataset) for training, verified to preserve generalizationâ€”confirmed by testing the final model on the full dataset with consistent loss metrics.  
   - Implements memory cleanup with `gc.collect()` during full-dataset inference.

3.# Architecture & Training 
   - **Autoencoder**: 4-layer Conv2D encoder (64â†’512 channels) + symmetric ConvTranspose2D decoder with BatchNorm and LeakyReLU.  
   - **Latent Space**: Optimal 4096-dimension latent space (experimentally determined for minimal loss vs training speed).  
   - **Normalization**: Inputs rescaled to [0,1] and sparse pixels (<1e-6) augmented with noise via `HandleSparseImages` to avoid anomalies.  
   - **LR Scheduling**: `ReduceLROnPlateau` with patience=5 to dynamically adjust learning rate.


Sure! Here's your content formatted nicely as a **README.md** section, perfect for GitHub or documentation purposes:

---

# Jet Image Autoencoder

## ðŸ“Œ Custom Weighted MSE Loss
- Implements a custom loss function: `WeightedMSE(weight_nonzero=1000.0)`, emphasizing non-zero pixel reconstruction, which is crucial for handling **sparse jet images**.
- Non-zero pixels are weighted **1000x more** than zero pixels to balance the loss between sparse and dense image regions.

## âš™ï¸ Efficient Data Handling
- Data loading and preprocessing are parallelized using Pythonâ€™s `ThreadPoolExecutor`, utilizing up to `min(8, os.cpu_count())` workers.
- Uses a **subset of 30,000 samples** (from the full dataset) for training, with extensive testing confirming it **preserves generalization**â€”validated by consistent loss on the full dataset.
- Applies memory management techniques such as `gc.collect()` during inference on the full dataset to reduce memory footprint.

## ðŸ§  Architecture & Training
- **Autoencoder**:
  - Encoder: 4-layer Conv2D network with channel progression from 64 to 512.
  - Decoder: Symmetric ConvTranspose2D layers with **BatchNorm** and **LeakyReLU** activations.
- **Latent Space**:
  - 4096-dimensional latent vector found to be optimalâ€”balancing low reconstruction loss and training speed.
- **Input Normalization**:
  - Inputs scaled to [0, 1].
  - Sparse pixels (< 1e-6) are augmented with noise via `HandleSparseImages` to prevent numerical instability.
- **Learning Rate Scheduler**:
  - Uses `ReduceLROnPlateau` with a patience of 5 epochs to adaptively tune the learning rate during training.

---

Let me know if you want to include usage instructions, installation, visuals, or experiment results too!
